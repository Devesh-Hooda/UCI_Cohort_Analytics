# -*- coding: utf-8 -*-
"""UCI_EDA_Analytics.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AEwxbZrSDr_2_TwkAopdfGPARDWbQh2-

### 1. Data Loading and Initial Inspection

This section handles uploading the dataset and loading it into a pandas DataFrame. It then performs initial checks to understand the data's structure and contents.
"""

from google.colab import files
uploaded = files.upload()

import pandas as pd

# Replace the filename with the uploaded file's name
df = pd.read_excel(list(uploaded.keys())[0])

df.head()
df.info()
df.describe(include='all')
df.isnull().sum()
df.head(10)

#===============================================================================
# CREATE NEW COLUMNS AND CONVER EXISTING DATA COLUMNS

# Convert InvoiceDate to datetime
df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'])

# Create Revenue column
df['Revenue'] = df['Quantity'] * df['Price']

#===============================================================================

df.info()
df.head()
df.isnull().sum()

"""### 2. Data Pre-processing and validity

"""

import pandas as pd
import numpy as np

# ---------------------------------------
# Drop rows with missing Customer ID
# ---------------------------------------

df_clean = df.dropna(subset=['Customer ID']).copy()

# Convert Customer ID to integer (remove decimal .0)
df_clean['Customer ID'] = df_clean['Customer ID'].astype(int)

# ---------------------------------------
# Handle missing product descriptions
# ---------------------------------------
# If Description is missing, drop (very small percentage)

df_clean = df_clean.dropna(subset=['Description'])
df_clean['Description'] = df_clean['Description'].str.strip().str.upper() # Trim whitespace


# ---------------------------------------
# Handle negative quantities (returns)
# ---------------------------------------
# Remove negative quantities entirely

df_clean = df_clean[df_clean['Quantity'] > 0]

# ---------------------------------------
# Remove rows with invalid StockCodes
# Common invalid codes include: POST, D, C2, BANK CHARGES, etc.
# ---------------------------------------
invalid_codes = ['POST', 'D', 'C2', 'BANK CHARGES', 'M', 'DOT']
df_clean = df_clean[~df_clean['StockCode'].isin(invalid_codes)]

# ---------------------------------------
# Remove duplicate rows (if any)
# ---------------------------------------

df_clean = df_clean.drop_duplicates()

# ---------------------------------------
# Verify final datatypes
# ---------------------------------------

print(df_clean.info())
print(df_clean.head())

# ---------------------------------------
# Find the sum of null columns
# ---------------------------------------

print(df_clean.isnull().sum())

# Cell 1 — imports and base settings
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from matplotlib.dates import DateFormatter
import plotly.express as px
import plotly.graph_objects as go

# Optional: nicer seaborn style for static plots
sns.set_style("whitegrid")
plt.rcParams['figure.figsize'] = (12,6)

# Cell 2 — quick sanity checks (ensure df_clean exists)
print("Rows:", len(df_clean))
print("Date range:", df_clean['InvoiceDate'].min(), "to", df_clean['InvoiceDate'].max())
print("Total revenue (sum):", df_clean['Revenue'].sum())

# ---------------------------
# Monthly Revenue & Trend
# ---------------------------

# Create month column (period) and monthly aggregates
df_clean['InvoiceMonth'] = df_clean['InvoiceDate'].dt.to_period('M').dt.to_timestamp()
monthly = df_clean.groupby('InvoiceMonth').agg({
    'Revenue': 'sum',
    'Invoice': pd.Series.nunique,   # number of orders (invoices)
    'Customer ID': pd.Series.nunique
}).rename(columns={'Invoice': 'NumOrders', 'Customer ID': 'NumCustomers'}).reset_index()

# Rolling mean for smoothing
monthly['Revenue_Rolling_3'] = monthly['Revenue'].rolling(3, min_periods=1).mean()

# Plot: Monthly Revenue + rolling
fig, ax = plt.subplots(figsize=(14,6))
ax.plot(monthly['InvoiceMonth'], monthly['Revenue'], marker='o', label='Monthly Revenue')
ax.plot(monthly['InvoiceMonth'], monthly['Revenue_Rolling_3'], linestyle='--', label='3-month rolling')
ax.set_title("Monthly Revenue and 3-month Rolling Average")
ax.set_xlabel("Month")
ax.set_ylabel("Revenue (GBP)")
ax.xaxis.set_major_formatter(DateFormatter("%Y-%m"))
ax.legend()
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

fig = px.line(monthly, x='InvoiceMonth', y='Revenue', title='Monthly Revenue (interactive)', markers=True)
fig.add_scatter(x=monthly['InvoiceMonth'], y=monthly['Revenue_Rolling_3'], mode='lines', name='3M Rolling')
fig.update_xaxes(tickformat="%Y-%m")
fig.show()

# ---------------------------
# Top Products (by Revenue & Quantity)
# ---------------------------

# Product-level aggregations
prod_agg = df_clean.groupby(['StockCode','Description']).agg({
    'Revenue': 'sum',
    'Quantity': 'sum',
    'Invoice': pd.Series.nunique
}).rename(columns={'Invoice':'NumOrders'}).reset_index()

# Top 20 by Revenue
top20_rev = prod_agg.sort_values('Revenue', ascending=False).head(20)

# Top 20 by Quantity
top20_qty = prod_agg.sort_values('Quantity', ascending=False).head(20)


fig = px.bar(top20_rev,
             x='Revenue',
             y='Description',
             title='Top 20 Products by Revenue (Interactive)',
             orientation='h',
             labels={'Revenue': 'Total Revenue (GBP)', 'Description': 'Product Description'})
fig.update_layout(yaxis={'categoryorder':'total ascending'})
fig.show()

fig = px.bar(top20_qty,
             x='Quantity',
             y='Description',
             title='Top 20 Products by Quantity Sold (Interactive)',
             orientation='h',
             labels={'Quantity': 'Total Quantity Sold', 'Description': 'Product Description'})
fig.update_layout(yaxis={'categoryorder':'total ascending'})
fig.show()

# ---------------------------
# Cohort Analysis — Monthly Cohorts (Retention)
# ---------------------------

# 1) Identify each customer's first purchase month
df_clean['InvoiceMonth'] = df_clean['InvoiceDate'].dt.to_period('M').dt.to_timestamp()
cust_first = df_clean.groupby('Customer ID')['InvoiceMonth'].min().reset_index()
cust_first.columns = ['Customer ID', 'CohortMonth']

# 2) Attach cohort to original df
df_cohort = df_clean.merge(cust_first, on='Customer ID')

# 3) Compute period number (months since cohort)
df_cohort['CohortIndex'] = ((df_cohort['InvoiceMonth'].dt.year - df_cohort['CohortMonth'].dt.year) * 12 +
                             (df_cohort['InvoiceMonth'].dt.month - df_cohort['CohortMonth'].dt.month)) + 1

# 4) Build cohort table: number of unique customers per cohort by period
cohort_counts = df_cohort.groupby(['CohortMonth', 'CohortIndex'])['Customer ID'].nunique().reset_index()
cohort_pivot = cohort_counts.pivot(index='CohortMonth', columns='CohortIndex', values='Customer ID')

# 5) Divide by cohort size to get retention rates
cohort_size = cohort_pivot.iloc[:,0]
retention = cohort_pivot.divide(cohort_size, axis=0).round(3)

# Display retention matrix (first 12 months)
display(retention.iloc[:,:12])

plt.figure(figsize=(12,8))
sns.heatmap(retention.iloc[:,:12], annot=True, fmt=".0%", cmap="YlGnBu")
plt.title("Cohort Retention Rate (by Cohort Month) — First 12 Periods")
plt.xlabel("Cohort Period (months since first purchase)")
plt.ylabel("Cohort Month")
plt.yticks(rotation=0)
plt.tight_layout()
plt.show()

# ---------------------------
# Exports for Tableau
# ---------------------------

# Company summary (monthly)
monthly.to_csv('company_monthly_summary.csv', index=False)

# Product summary
prod_agg.to_csv('product_summary.csv', index=False)

# Cohort retention (export retention numeric values)
retention.reset_index().to_csv('cohort_retention.csv', index=False)

print("Exported: company_monthly_summary.csv, product_summary.csv, cohort_retention.csv")

from google.colab import files

# Export the cleaned main dataset
df_clean.to_csv("cleaned_online_retail.csv", index=False)

files.download("cleaned_online_retail.csv")

# Export monthly summary
monthly.to_csv("company_monthly_summary.csv", index=False)
files.download("company_monthly_summary.csv")

# Export product-level summary
prod_agg.to_csv("product_summary.csv", index=False)
files.download("product_summary.csv")

# Export cohort retention (retention matrix)
retention.reset_index().to_csv("cohort_retention.csv", index=False)
files.download("cohort_retention.csv")

import pandas as pd
import numpy as np

# RECENCY REFERENCE DATE
snapshot_date = df_clean['InvoiceDate'].max() + pd.Timedelta(days=1)
print("Snapshot Date:", snapshot_date)

# Group by Customer ID
rfm = df_clean.groupby('Customer ID').agg({
    'InvoiceDate': lambda x: (snapshot_date - x.max()).days,  # Recency
    'Invoice':     'nunique',                                 # Frequency
    'Revenue':     'sum'                                      # Monetary
})

# Rename columns
rfm.columns = ['Recency', 'Frequency', 'Monetary']

rfm = rfm.reset_index()
rfm.head()

# Recency score (lower recency = better customer)
rfm['R_score'] = pd.qcut(rfm['Recency'], 5, labels=[5,4,3,2,1]).astype(int)

# Frequency & Monetary (higher is better)
rfm['F_score'] = pd.qcut(rfm['Frequency'].rank(method='first'), 5, labels=[1,2,3,4,5]).astype(int)
rfm['M_score'] = pd.qcut(rfm['Monetary'].rank(method='first'), 5, labels=[1,2,3,4,5]).astype(int)

rfm['RFM_Score'] = rfm['R_score'].astype(str) + rfm['F_score'].astype(str) + rfm['M_score'].astype(str)
rfm.head()

def rfm_segment(row):
    r = row['R_score']
    f = row['F_score']
    m = row['M_score']

    if r >= 4 and f >= 4 and m >= 4:
        return 'Best Customers'
    if f >= 4 and m >= 3:
        return 'Loyal Customers'
    if r >= 3 and f >= 3:
        return 'Potential Loyalists'
    if r <= 2 and f <= 2:
        return 'At Risk'
    return 'Others'

rfm['Segment'] = rfm.apply(rfm_segment, axis=1)

rfm['Segment'].value_counts()

import seaborn as sns
import matplotlib.pyplot as plt

fig, ax = plt.subplots(1,3, figsize=(18,5))
sns.histplot(rfm['Recency'], ax=ax[0], kde=True)
sns.histplot(rfm['Frequency'], ax=ax[1], kde=True)
sns.histplot(rfm['Monetary'], ax=ax[2], kde=True)

ax[0].set_title("Recency Distribution")
ax[1].set_title("Frequency Distribution")
ax[2].set_title("Monetary Distribution")
plt.show()

rfm.to_csv("rfm_table.csv", index=False)

"""RFM Export"""

from google.colab import files
files.download("rfm_table.csv")

"""### Overall Data and Business Health

1.  **Data Quality**: The initial dataset had missing values, particularly in `Description` and `Customer ID`, and contained invalid entries like negative quantities and miscellaneous stock codes. These issues were addressed through a robust pre-processing step, resulting in a clean dataset (`df_clean`) of 399,643 valid transactions.
2.  **Revenue Performance**: The total revenue generated over the analyzed period (December 2009 to December 2010) was approximately £8.64 million. The monthly revenue trend showed fluctuations, but a 3-month rolling average helped smooth out short-term variations, providing a clearer view of overall growth or stability.

### Product Insights

1.  **Top Products**: Analysis of product performance by total revenue and quantity sold identified key items driving the business. This insight is crucial for inventory management, marketing campaigns, and understanding customer preferences.

### Customer Behavior and Loyalty

1.  **Customer Retention**: The cohort analysis provided a detailed view of customer retention rates over time. It showed how many customers from each initial purchasing cohort returned in subsequent months. This is vital for understanding customer loyalty and the effectiveness of retention strategies.
2.  **Customer Segmentation (RFM)**: Through RFM (Recency, Frequency, Monetary) analysis, customers were segmented into distinct groups such as 'Best Customers', 'Loyal Customers', 'Potential Loyalists', and 'At Risk' customers. This segmentation offers actionable insights for targeted marketing efforts:
    *   **Best Customers**: Likely high-value, recent, and frequent purchasers; these are crucial for continued growth.
    *   **Loyal Customers**: Frequent purchasers with good monetary value; focus on maintaining their engagement.
    *   **Potential Loyalists**: Recent and moderately frequent/monetary customers; can be nurtured to become loyal.
    *   **At Risk**: Customers who haven't purchased recently or frequently; require re-engagement strategies.

### Conclusion
The project provided a comprehensive view of the e-commerce business's performance, from data cleanliness to intricate customer behavior patterns. The insights gained from monthly revenue trends, top product identification, cohort retention, and RFM segmentation offer a strong foundation for strategic decision-making in areas like marketing, customer relationship management, and inventory optimization.
"""